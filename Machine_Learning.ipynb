{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below will be a basic model including all of the features I collected for school districts. As mentioned in the Data Analysis notebook, I beleive the model will suffer from multicollinearity. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  OLS Regression Results                                 \n",
      "=========================================================================================\n",
      "Dep. Variable:     Graduated 4-Year (%)   R-squared (uncentered):                   0.960\n",
      "Model:                              OLS   Adj. R-squared (uncentered):              0.959\n",
      "Method:                   Least Squares   F-statistic:                              744.0\n",
      "Date:                  Fri, 13 Mar 2020   Prob (F-statistic):                   4.51e-250\n",
      "Time:                          16:28:37   Log-Likelihood:                         -1349.1\n",
      "No. Observations:                   381   AIC:                                      2722.\n",
      "Df Residuals:                       369   BIC:                                      2770.\n",
      "Df Model:                            12                                                  \n",
      "Covariance Type:              nonrobust                                                  \n",
      "===================================================================================================\n",
      "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ACT-Composite                       2.2297      0.626      3.563      0.000       0.999       3.460\n",
      "ACT-Part_Rate                       0.0435      0.039      1.124      0.262      -0.033       0.120\n",
      "AP-11&12 Participating Students    -0.0030      0.004     -0.677      0.499      -0.012       0.006\n",
      "AP-Total Exams                      0.0024      0.002      1.530      0.127      -0.001       0.006\n",
      "AP-Passed(%)                        0.3047      0.039      7.852      0.000       0.228       0.381\n",
      "AP-Exams Taken Per Student         -5.5846      1.655     -3.375      0.001      -8.839      -2.331\n",
      "Enrolled 4-Year                     0.0076      0.006      1.272      0.204      -0.004       0.019\n",
      "Total Graduated                    -0.0038      0.002     -2.305      0.022      -0.007      -0.001\n",
      "Enrolled 4-Year (%)                -0.4285      0.135     -3.169      0.002      -0.694      -0.163\n",
      "SAT-Total                           0.0009      0.012      0.073      0.942      -0.022       0.024\n",
      "SAT-Part_Rate                      -0.0274      0.043     -0.636      0.525      -0.112       0.057\n",
      "Wealth/ADA                      -9.625e-07   2.56e-06     -0.376      0.707   -5.99e-06    4.07e-06\n",
      "==============================================================================\n",
      "Omnibus:                       10.559   Durbin-Watson:                   1.973\n",
      "Prob(Omnibus):                  0.005   Jarque-Bera (JB):               15.096\n",
      "Skew:                           0.215   Prob(JB):                     0.000527\n",
      "Kurtosis:                       3.875   Cond. No.                     1.78e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.78e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "### Interpreting Coefficients, Stats Model OLS ###\n",
    "from statsmodels.api import OLS\n",
    "grad = pd.read_csv('Feature_Target_Data.csv')\n",
    "years = [2011, 2012, 2013, 2014]\n",
    "grad = grad.loc[grad['Year'].isin(years)]\n",
    "grad = grad[grad.columns[3:]]\n",
    "X = grad[grad.columns[:-1]]\n",
    "y = grad[grad.columns[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)\n",
    "olsreg = OLS(y_train, X_train)\n",
    "olsreg = olsreg.fit()\n",
    "print(olsreg.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "As seen from Warning 2, the basic model is indeed dealing with multicollinearity. \n",
    "\n",
    "From Data Analysis notebook, below are features with that contain high correlations with other features that we can consider dropping to benefit our regression model\n",
    "\n",
    "['AP-Total Exams', 'AP-Passed(%)', 'Enrolled 4-Year', 'Total Graduated', 'SAT-Total']\n",
    "\n",
    "When thinking from a significance standpoint, we could also explore excluding 'Wealth/ADA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  OLS Regression Results                                 \n",
      "=========================================================================================\n",
      "Dep. Variable:     Graduated 4-Year (%)   R-squared (uncentered):                   0.959\n",
      "Model:                              OLS   Adj. R-squared (uncentered):              0.958\n",
      "Method:                   Least Squares   F-statistic:                              1448.\n",
      "Date:                  Fri, 13 Mar 2020   Prob (F-statistic):                   7.61e-256\n",
      "Time:                          16:28:41   Log-Likelihood:                         -1357.1\n",
      "No. Observations:                   381   AIC:                                      2726.\n",
      "Df Residuals:                       375   BIC:                                      2750.\n",
      "Df Model:                             6                                                  \n",
      "Covariance Type:              nonrobust                                                  \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "ACT-Composite                  2.0954      0.154     13.586      0.000       1.792       2.399\n",
      "ACT-Part_Rate                  0.0679      0.037      1.851      0.065      -0.004       0.140\n",
      "AP-Passed(%)                   0.3358      0.033     10.198      0.000       0.271       0.401\n",
      "AP-Exams Taken Per Student    -5.8357      1.361     -4.286      0.000      -8.513      -3.159\n",
      "Enrolled 4-Year (%)           -0.3652      0.114     -3.202      0.001      -0.589      -0.141\n",
      "SAT-Part_Rate                 -0.0395      0.039     -1.008      0.314      -0.117       0.038\n",
      "==============================================================================\n",
      "Omnibus:                       12.775   Durbin-Watson:                   1.949\n",
      "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               20.029\n",
      "Skew:                           0.230   Prob(JB):                     4.48e-05\n",
      "Kurtosis:                       4.024   Cond. No.                         274.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "### Studying Effects of Dropping Certain Features Due to Multicollinearity/Domain Insignificance ###\n",
    "grad = grad.drop(['AP-Total Exams', 'Enrolled 4-Year', 'Total Graduated', 'AP-11&12 Participating Students', 'SAT-Total', 'Wealth/ADA'], axis=1)\n",
    "X = grad[grad.columns[:-1]]\n",
    "y = grad[grad.columns[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)\n",
    "olsreg = OLS(y_train, X_train)\n",
    "olsreg = olsreg.fit()\n",
    "print(olsreg.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Warning about Multicollinearity is gone ^, significance of features has gotten better as well.\n",
    "Below we'll test out linear regression in scikit learn and view resulting RMSE, r2, and MAPE to see if the model how much better the model performs with all features and without the features causing multicollinearity/insignificant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_size:  0.325\n",
      "---All Features--- \n",
      "\n",
      "Training RMSE: 8.22427289729042 Testing RMSE: 7.076394216935626 Difference: 1.1478786803547942\n",
      "\n",
      "Training R2:  0.5613787027319719 Testing R2:  0.6865052072040615 Difference: 0.12512650447208962\n",
      "\n",
      "Training MAPE:  18.191129263242825 Testing MAPE:  15.905683997719485 Difference: 2.2854452655233395\n",
      "\n",
      " ---After Dropping Features--- \n",
      "\n",
      "Training RMSE: 8.510824902215692 Testing RMSE: 6.964779072266455 Difference: 1.5460458299492368\n",
      "\n",
      "Training R2:  0.5302811368951164 Testing R2:  0.6963166487196456 Difference: 0.16603551182452925\n",
      "\n",
      "Training MAPE:  18.619150250025406 Testing MAPE:  15.643188970542155 Difference: 2.9759612794832506\n",
      "\n",
      "test_size:  0.3\n",
      "---All Features--- \n",
      "\n",
      "Training RMSE: 8.125371609673968 Testing RMSE: 7.211472957627714 Difference: 0.913898652046254\n",
      "\n",
      "Training R2:  0.5759426378754136 Testing R2:  0.6694028154431887 Difference: 0.09346017756777514\n",
      "\n",
      "Training MAPE:  17.902627557682482 Testing MAPE:  16.289719551681827 Difference: 1.612908006000655\n",
      "\n",
      " ---After Dropping Features--- \n",
      "\n",
      "Training RMSE: 8.41204727124919 Testing RMSE: 7.071585657705524 Difference: 1.340461613543666\n",
      "\n",
      "Training R2:  0.5454919790695266 Testing R2:  0.6821041892645017 Difference: 0.13661221019497505\n",
      "\n",
      "Training MAPE:  18.36607026574389 Testing MAPE:  15.863972360805592 Difference: 2.5020979049382976\n",
      "\n",
      "test_size:  0.275\n",
      "---All Features--- \n",
      "\n",
      "Training RMSE: 8.017931023117596 Testing RMSE: 7.412340656368568 Difference: 0.6055903667490279\n",
      "\n",
      "Training R2:  0.5836454997010633 Testing R2:  0.6576864364615267 Difference: 0.07404093676046342\n",
      "\n",
      "Training MAPE:  17.619102890923443 Testing MAPE:  16.79636763010195 Difference: 0.8227352608214922\n",
      "\n",
      " ---After Dropping Features--- \n",
      "\n",
      "Training RMSE: 8.314362122165276 Testing RMSE: 7.22940990827092 Difference: 1.084952213894356\n",
      "\n",
      "Training R2:  0.5522903006390456 Testing R2:  0.6743740057206836 Difference: 0.12208370508163802\n",
      "\n",
      "Training MAPE:  18.19068570940845 Testing MAPE:  15.99036814217823 Difference: 2.2003175672302184\n",
      "\n",
      "test_size:  0.25\n",
      "---All Features--- \n",
      "\n",
      "Training RMSE: 8.039580002590322 Testing RMSE: 7.270472881050807 Difference: 0.7691071215395144\n",
      "\n",
      "Training R2:  0.5845709858118027 Testing R2:  0.6667216573402157 Difference: 0.08215067152841304\n",
      "\n",
      "Training MAPE:  17.648364646323063 Testing MAPE:  16.839248356026104 Difference: 0.8091162902969593\n",
      "\n",
      " ---After Dropping Features--- \n",
      "\n",
      "Training RMSE: 8.340827253346166 Testing RMSE: 7.012582803482295 Difference: 1.3282444498638712\n",
      "\n",
      "Training R2:  0.552855023741984 Testing R2:  0.6899456848554524 Difference: 0.13709066111346835\n",
      "\n",
      "Training MAPE:  18.27036806499571 Testing MAPE:  15.734016811248416 Difference: 2.536351253747295\n",
      "\n",
      "test_size:  0.225\n",
      "---All Features--- \n",
      "\n",
      "Training RMSE: 8.072430731448717 Testing RMSE: 7.015122792465307 Difference: 1.0573079389834108\n",
      "\n",
      "Training R2:  0.5840795528271145 Testing R2:  0.6815744801015764 Difference: 0.09749492727446185\n",
      "\n",
      "Training MAPE:  17.7830364262171 Testing MAPE:  15.888292610444903 Difference: 1.8947438157721983\n",
      "\n",
      " ---After Dropping Features--- \n",
      "\n",
      "Training RMSE: 8.341862092262536 Testing RMSE: 6.8259774168000416 Difference: 1.5158846754624946\n",
      "\n",
      "Training R2:  0.5558520838626373 Testing R2:  0.698514099673499 Difference: 0.1426620158108617\n",
      "\n",
      "Training MAPE:  18.308234631588256 Testing MAPE:  15.007234359465105 Difference: 3.301000272123151\n"
     ]
    }
   ],
   "source": [
    "########## Linear Regression Model for Predicting Graduated 4-Year (%) ############\n",
    "### ALL Features ###\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "for test_size in [.325, .3, .275, .25, .225]:\n",
    "    print('\\ntest_size: ', test_size)\n",
    "    grad = pd.read_csv('Feature_Target_Data.csv')\n",
    "    years = [2011, 2012, 2013, 2014]\n",
    "    grad = grad.loc[grad['Year'].isin(years)]\n",
    "    grad = grad[grad.columns[3:]]\n",
    "    X = grad.drop('Graduated 4-Year (%)', axis=1).values\n",
    "    y = grad['Graduated 4-Year (%)'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    y_pred_test = linear_reg.predict(X_test)\n",
    "    y_pred_train = linear_reg.predict(X_train)\n",
    "    print('---All Features---', '\\n')\n",
    "    print('Training RMSE:', np.sqrt(mean_squared_error(y_train, y_pred_train)), 'Testing RMSE:', np.sqrt(mean_squared_error(y_test, y_pred_test)), 'Difference:', abs(np.sqrt(mean_squared_error(y_train, y_pred_train)) - np.sqrt(mean_squared_error(y_test, y_pred_test))))\n",
    "    print('\\nTraining R2: ', linear_reg.score(X_train, y_train), 'Testing R2: ', linear_reg.score(X_test, y_test), 'Difference:', abs(linear_reg.score(X_train, y_train) - linear_reg.score(X_test, y_test)))\n",
    "    print('\\nTraining MAPE: ', mean_absolute_percentage_error(y_train, y_pred_train), 'Testing MAPE: ', mean_absolute_percentage_error(y_test, y_pred_test), 'Difference:', abs(mean_absolute_percentage_error(y_train, y_pred_train) - mean_absolute_percentage_error(y_test, y_pred_test)))\n",
    "\n",
    "### After Dropping Features Causing Mulicollinearity/Insignificance ###\n",
    "    X = grad.drop(['Graduated 4-Year (%)', 'AP-Total Exams', 'Enrolled 4-Year', 'Total Graduated', 'AP-11&12 Participating Students', 'SAT-Total', 'Wealth/ADA'], axis=1).values\n",
    "    y = grad['Graduated 4-Year (%)'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    y_pred_test = linear_reg.predict(X_test)\n",
    "    y_pred_train = linear_reg.predict(X_train)\n",
    "    print('\\n', '---After Dropping Features---', '\\n')\n",
    "    print('Training RMSE:', np.sqrt(mean_squared_error(y_train, y_pred_train)), 'Testing RMSE:', np.sqrt(mean_squared_error(y_test, y_pred_test)), 'Difference:', abs(np.sqrt(mean_squared_error(y_train, y_pred_train)) - np.sqrt(mean_squared_error(y_test, y_pred_test))))\n",
    "    print('\\nTraining R2: ', linear_reg.score(X_train, y_train), 'Testing R2: ', linear_reg.score(X_test, y_test), 'Difference:', abs(linear_reg.score(X_train, y_train) - linear_reg.score(X_test, y_test)))\n",
    "    print('\\nTraining MAPE: ', mean_absolute_percentage_error(y_train, y_pred_train), 'Testing MAPE: ', mean_absolute_percentage_error(y_test, y_pred_test), 'Difference:', abs(mean_absolute_percentage_error(y_train, y_pred_train) - mean_absolute_percentage_error(y_test, y_pred_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Every metric above appears to improve for the testing set, indicating a better performing model on unseen data in predicting the target (Graduation 4-Year %). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
